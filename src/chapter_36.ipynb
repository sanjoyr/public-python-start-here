{
 "cells": [
  {
   "cell_type": "markdown",
   "execution_count": null,
   "id": "920d1563",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Chapter 36: Why Accuracy Misleads\n",
    "\n",
    "⚠️ **DO NOT SKIP THIS CELL**\n",
    "\n",
    "## Run the Next cell.\n",
    "### Before executing any other cell you must run the next cell to set up the project folder environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1140852c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "try:\n",
    "    from google.colab import drive\n",
    "    IN_COLAB = True\n",
    "except ImportError:\n",
    "    IN_COLAB = False\n",
    "\n",
    "if IN_COLAB:\n",
    "    drive.mount(\"/content/drive\")\n",
    "    PROJECT_ROOT = Path(\"/content/drive/MyDrive/DataScience/census-education-analysis\")\n",
    "else:\n",
    "    PROJECT_ROOT = Path.cwd().parent\n",
    "\n",
    "DATA_DIR = PROJECT_ROOT / \"data\"\n",
    "RAW_DIR = DATA_DIR / \"raw\"\n",
    "STAGING_DIR = DATA_DIR / \"staging\"\n",
    "PROCESSED_DIR = DATA_DIR / \"processed\"\n",
    "OUTPUTS_DIR = PROJECT_ROOT / \"outputs\"\n",
    "\n",
    "PROJECT_ROOT\n"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "id": "3973007e",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Problem 1: What Dataset Are We Evaluating?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa33fb14",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "input_path = OUTPUTS_DIR / \"india_model_scored.csv\"\n",
    "df = pd.read_csv(input_path)\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "id": "7c0bc54c",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Problem 2: What Does Accuracy Actually Measure?"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "id": "99135f2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Problem 3: Why Can a Model Be Accurate and Still Useless?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ece3779a",
   "metadata": {},
   "outputs": [],
   "source": [
    "baseline_accuracy = (df[\"priority_flag\"] == False).mean()\n",
    "baseline_accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "id": "3656b4f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Problem 4: What Are False Positives and False Negatives in Decision Terms?"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "id": "1b5af046",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Problem 5: Why Unequal Error Costs Break Accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "id": "73245519",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Problem 6: How Does Accuracy Hide Structural Bias?"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "id": "b2012e73",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Problem 7: What Should Analysts Evaluate Instead?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d0daca3",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.sort_values(\"risk_score\", ascending=False).head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "id": "6662beab",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Problem 8: Comparing Model Behavior to the Reference Decision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39366bf5",
   "metadata": {},
   "outputs": [],
   "source": [
    "threshold = 0.5\n",
    "df[\"predicted_flag\"] = df[\"risk_score\"] >= threshold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8c27113",
   "metadata": {},
   "outputs": [],
   "source": [
    "disagreements = df[df[\"predicted_flag\"] != df[\"priority_flag\"]]\n",
    "disagreements.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "id": "c61e64c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Problem 9: Saving Evaluation-Ready Data for Interpretation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69c7b318",
   "metadata": {},
   "outputs": [],
   "source": [
    "output_path = OUTPUTS_DIR / \"india_model_evaluation_ready.csv\"\n",
    "df.to_csv(output_path, index=False)\n",
    "\n",
    "output_path"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "id": "88d402fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "## End-of-Chapter Direction"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
