{
 "cells": [
  {
   "cell_type": "markdown",
   "execution_count": null,
   "id": "3b563590",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Chapter 35: Building a Responsible First Model\n",
    "\n",
    "⚠️ **DO NOT SKIP THIS CELL**\n",
    "\n",
    "## Run the Next cell.\n",
    "### Before executing any other cell you must run the next cell to set up the project folder environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddda6e6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "try:\n",
    "    from google.colab import drive\n",
    "    IN_COLAB = True\n",
    "except ImportError:\n",
    "    IN_COLAB = False\n",
    "\n",
    "if IN_COLAB:\n",
    "    drive.mount(\"/content/drive\")\n",
    "    PROJECT_ROOT = Path(\"/content/drive/MyDrive/DataScience/census-education-analysis\")\n",
    "else:\n",
    "    PROJECT_ROOT = Path.cwd().parent\n",
    "\n",
    "DATA_DIR = PROJECT_ROOT / \"data\"\n",
    "RAW_DIR = DATA_DIR / \"raw\"\n",
    "STAGING_DIR = DATA_DIR / \"staging\"\n",
    "PROCESSED_DIR = DATA_DIR / \"processed\"\n",
    "OUTPUTS_DIR = PROJECT_ROOT / \"outputs\"\n",
    "\n",
    "PROJECT_ROOT\n"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "id": "85db708f",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Problem 1: What Dataset Are We Starting From?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "721b36c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "input_path = OUTPUTS_DIR / \"india_feature_ready.csv\"\n",
    "df = pd.read_csv(input_path)\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "id": "d3b5ffbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Problem 2: What Decision Are We Trying to Support?"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "id": "faee5e01",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Problem 3: Why Choose the Simplest Possible Model?"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "id": "3c997e39",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Problem 4: How Do We Separate Training from Testing (Conceptually)?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37847e38",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "features = [\n",
    "    \"literacy_rate\",\n",
    "    \"gender_literacy_gap\",\n",
    "    \"total_persons\"\n",
    "]\n",
    "\n",
    "X = df[features]\n",
    "y = df[\"priority_flag\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58c45c96",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X,\n",
    "    y,\n",
    "    test_size=0.2,\n",
    "    random_state=42\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "id": "86b53ab8",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Problem 5: What Kind of Model Fits This Goal?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c35e6972",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "model = LogisticRegression()\n",
    "model.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "id": "5c97b41d",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Problem 6: What Does the Model Actually Produce?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cac40b5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"risk_score\"] = model.predict_proba(X)[:, 1]"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "id": "a3130fec",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Problem 7: How Do We Evaluate Usefulness (Not Just Accuracy)?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88e3bf3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.sort_values(\"risk_score\", ascending=False).head()"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "id": "1c87b1b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Problem 8: How Do We Keep Humans in the Loop?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55228da4",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.sort_values(\"risk_score\", ascending=False).head()"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "id": "fdab11c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Problem 9: Saving the Model Output for the Next Chapter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d136e223",
   "metadata": {},
   "outputs": [],
   "source": [
    "output_path = OUTPUTS_DIR / \"india_model_scored.csv\"\n",
    "df.to_csv(output_path, index=False)\n",
    "\n",
    "output_path"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "id": "1ef36617",
   "metadata": {},
   "outputs": [],
   "source": [
    "## End-of-Chapter Direction"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
