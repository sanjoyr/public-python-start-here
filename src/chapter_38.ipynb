{
 "cells": [
  {
   "cell_type": "markdown",
   "execution_count": null,
   "id": "d8865b46",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Chapter 38: Turning Scores into Decisions\n",
    "\n",
    "⚠️ **DO NOT SKIP THIS CELL**\n",
    "\n",
    "## Run the Next cell.\n",
    "### Before executing any other cell you must run the next cell to set up the project folder environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6bccc030",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "try:\n",
    "    from google.colab import drive\n",
    "    IN_COLAB = True\n",
    "except ImportError:\n",
    "    IN_COLAB = False\n",
    "\n",
    "if IN_COLAB:\n",
    "    drive.mount(\"/content/drive\")\n",
    "    PROJECT_ROOT = Path(\"/content/drive/MyDrive/DataScience/census-education-analysis\")\n",
    "else:\n",
    "    PROJECT_ROOT = Path.cwd().parent\n",
    "\n",
    "DATA_DIR = PROJECT_ROOT / \"data\"\n",
    "RAW_DIR = DATA_DIR / \"raw\"\n",
    "STAGING_DIR = DATA_DIR / \"staging\"\n",
    "PROCESSED_DIR = DATA_DIR / \"processed\"\n",
    "OUTPUTS_DIR = PROJECT_ROOT / \"outputs\"\n",
    "\n",
    "PROJECT_ROOT\n"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "id": "5723bd3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Problem 1: What Dataset Are We Starting From?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9de3b0df",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "input_path = OUTPUTS_DIR / \"india_model_interpretable.csv\"\n",
    "df = pd.read_csv(input_path)\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "id": "23e37e05",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Problem 2: What Exactly Does a Model Give Us?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f596b58",
   "metadata": {},
   "outputs": [],
   "source": [
    "df[[\"state_name\", \"risk_score\"]].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "id": "2b5868df",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Problem 3: When Do Scores Become Decisions?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b038c73",
   "metadata": {},
   "outputs": [],
   "source": [
    "threshold = 0.70\n",
    "\n",
    "df[\"model_high_risk\"] = df[\"risk_score\"] >= threshold"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "id": "a3badebc",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Problem 4: How Should a Threshold Be Chosen?"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "id": "d9fe63a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Problem 5: Why Add Rules on Top of Model Scores?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea34c4c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"rule_override\"] = df[\"gender_literacy_gap\"] >= 0.15"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6070de1",
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"final_decision_flag\"] = (\n",
    "    df[\"model_high_risk\"] | df[\"rule_override\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "id": "51dfb09f",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Problem 6: Why Don’t Models Make Decisions by Themselves?"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "id": "f214f332",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Problem 7: Where Do Humans Fit in This Pipeline?"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "id": "8ad5e2e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Problem 8: What Does a Safe Decision Pipeline Look Like?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9aff6b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "df[df[\"final_decision_flag\"]].sort_values(\n",
    "    \"risk_score\", ascending=False\n",
    ").head()"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "id": "e25072d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Problem 9: Saving the Decision-Ready Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "267421ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "output_path = OUTPUTS_DIR / \"india_decision_ready.csv\"\n",
    "df.to_csv(output_path, index=False)\n",
    "\n",
    "output_path"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "id": "f28ba9f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "## End-of-Chapter Direction"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
