{
 "cells": [
  {
   "cell_type": "markdown",
   "execution_count": null,
   "id": "0a93f0a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Chapter 27: Ranking States\n",
    "\n",
    "⚠️ **DO NOT SKIP THIS CELL**\n",
    "\n",
    "## Run the Next cell.\n",
    "### Before executing any other cell you must run the next cell to set up the project folder environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5902fe0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "try:\n",
    "    from google.colab import drive\n",
    "    IN_COLAB = True\n",
    "except ImportError:\n",
    "    IN_COLAB = False\n",
    "\n",
    "if IN_COLAB:\n",
    "    drive.mount(\"/content/drive\")\n",
    "    PROJECT_ROOT = Path(\"/content/drive/MyDrive/DataScience/census-education-analysis\")\n",
    "else:\n",
    "    PROJECT_ROOT = Path.cwd().parent\n",
    "\n",
    "DATA_DIR = PROJECT_ROOT / \"data\"\n",
    "RAW_DIR = DATA_DIR / \"raw\"\n",
    "STAGING_DIR = DATA_DIR / \"staging\"\n",
    "PROCESSED_DIR = DATA_DIR / \"processed\"\n",
    "OUTPUTS_DIR = PROJECT_ROOT / \"outputs\"\n",
    "\n",
    "PROJECT_ROOT\n"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "id": "27365d40",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Problem 1: What Data Are We Ranking?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4726ef1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "state_path = PROCESSED_DIR / \"india_gender_metrics.csv\"\n",
    "state_df = pd.read_csv(state_path)\n",
    "\n",
    "state_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "id": "e5d3f0a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Problem 2: Why Must We Rank Derived Metrics, Not Raw Totals?"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "id": "8ff17a2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Problem 3: How Do We Rank States by Literacy Rate?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce6cab69",
   "metadata": {},
   "outputs": [],
   "source": [
    "ranked_by_literacy = (\n",
    "    state_df\n",
    "    .sort_values(\"literacy_rate\", ascending=False)\n",
    "    .reset_index(drop=True)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbdb7efc",
   "metadata": {},
   "outputs": [],
   "source": [
    "ranked_by_literacy.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "id": "071c8990",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Problem 4: How Do We Clearly Identify Top and Bottom Performers?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9473f486",
   "metadata": {},
   "outputs": [],
   "source": [
    "top_states = ranked_by_literacy.head(5)\n",
    "top_states"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8df3f887",
   "metadata": {},
   "outputs": [],
   "source": [
    "bottom_states = ranked_by_literacy.tail(5)\n",
    "bottom_states"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "id": "c189a460",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Problem 5: How Do Rankings Change When the Metric Changes?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39c00a16",
   "metadata": {},
   "outputs": [],
   "source": [
    "ranked_by_gender_gap = (\n",
    "    state_df\n",
    "    .sort_values(\"gender_literacy_gap\")\n",
    "    .reset_index(drop=True)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6d2ca3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "ranked_by_gender_gap.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fa4f47b",
   "metadata": {},
   "outputs": [],
   "source": [
    "ranked_by_gender_gap.tail()"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "id": "4f266446",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Problem 6: How Do We Avoid Misleading Rankings?"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "id": "22de0f91",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Problem 7: What New Questions Do Rankings Reveal?"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "id": "fd43aa85",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Problem 8: How Do We Save Ranked Results for the Next Chapter?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94cd9341",
   "metadata": {},
   "outputs": [],
   "source": [
    "ranked_output_path = PROCESSED_DIR / \"state_ranked_by_literacy.csv\"\n",
    "ranked_by_literacy.to_csv(ranked_output_path, index=False)\n",
    "\n",
    "ranked_output_path"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "id": "e0d8e949",
   "metadata": {},
   "outputs": [],
   "source": [
    "## End-of-Chapter Direction"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
