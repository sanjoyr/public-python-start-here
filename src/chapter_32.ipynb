{
 "cells": [
  {
   "cell_type": "markdown",
   "execution_count": null,
   "id": "15421566",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Chapter 32: When Do You Actually Need a Model?\n",
    "\n",
    "⚠️ **DO NOT SKIP THIS CELL**\n",
    "\n",
    "## Run the Next cell.\n",
    "### Before executing any other cell you must run the next cell to set up the project folder environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aca773ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "try:\n",
    "    from google.colab import drive\n",
    "    IN_COLAB = True\n",
    "except ImportError:\n",
    "    IN_COLAB = False\n",
    "\n",
    "if IN_COLAB:\n",
    "    drive.mount(\"/content/drive\")\n",
    "    PROJECT_ROOT = Path(\"/content/drive/MyDrive/DataScience/census-education-analysis\")\n",
    "else:\n",
    "    PROJECT_ROOT = Path.cwd().parent\n",
    "\n",
    "DATA_DIR = PROJECT_ROOT / \"data\"\n",
    "RAW_DIR = DATA_DIR / \"raw\"\n",
    "STAGING_DIR = DATA_DIR / \"staging\"\n",
    "PROCESSED_DIR = DATA_DIR / \"processed\"\n",
    "OUTPUTS_DIR = PROJECT_ROOT / \"outputs\"\n",
    "\n",
    "PROJECT_ROOT\n"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "id": "a5a2f4e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Problem 1: What Dataset Are We Starting From?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff486a56",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "ai_path = OUTPUTS_DIR / \"india_ai_ready.csv\"\n",
    "ai_df = pd.read_csv(ai_path)\n",
    "\n",
    "ai_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "id": "02765868",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Problem 2: What Decision Are We Trying to Make?"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "id": "a6e6d3e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Problem 3: Can a Simple Rule Solve This?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e59b5f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "ai_df[\"low_literacy_rule\"] = ai_df[\"literacy_rate\"] < 0.75"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "deee8faf",
   "metadata": {},
   "outputs": [],
   "source": [
    "ai_df[\"low_literacy_rule\"].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "id": "5ffec0a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Problem 4: When Do Thresholds Improve on Simple Rules?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "772be9fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "ai_df[\"high_gender_gap_threshold\"] = (\n",
    "    ai_df[\"gender_literacy_gap\"] > 0.10\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "id": "8f8f41fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Problem 5: How Do We Combine Multiple Simple Decisions?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad163694",
   "metadata": {},
   "outputs": [],
   "source": [
    "ai_df[\"priority_flag\"] = (\n",
    "    ai_df[\"low_literacy_rule\"] |\n",
    "    ai_df[\"high_gender_gap_threshold\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69398296",
   "metadata": {},
   "outputs": [],
   "source": [
    "ai_df[\"priority_flag\"].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "id": "9730af51",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Problem 6: When Do Simple Rules Start to Break?"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "id": "2a49bd40",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Problem 7: What Actually Changes When You Introduce a Model?"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "id": "6e5e2b5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Problem 8: Freezing Rule-Based Decisions for Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be0dcec7",
   "metadata": {},
   "outputs": [],
   "source": [
    "decision_df = ai_df[[\n",
    "    \"state_name\",\n",
    "    \"total_persons\",\n",
    "    \"literacy_rate\",\n",
    "    \"gender_literacy_gap\",\n",
    "    \"low_literacy_rule\",\n",
    "    \"high_gender_gap_threshold\",\n",
    "    \"priority_flag\"\n",
    "]].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de4da3f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "output_path = OUTPUTS_DIR / \"india_rule_based_decisions.csv\"\n",
    "decision_df.to_csv(output_path, index=False)\n",
    "\n",
    "output_path"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "id": "9b254ed8",
   "metadata": {},
   "outputs": [],
   "source": [
    "## End-of-Chapter Direction"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
