{
 "cells": [
  {
   "cell_type": "markdown",
   "execution_count": null,
   "id": "8f86c4b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Chapter 37: Interpreting Models Clearly\n",
    "\n",
    "⚠️ **DO NOT SKIP THIS CELL**\n",
    "\n",
    "## Run the Next cell.\n",
    "### Before executing any other cell you must run the next cell to set up the project folder environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90cfe4fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "try:\n",
    "    from google.colab import drive\n",
    "    IN_COLAB = True\n",
    "except ImportError:\n",
    "    IN_COLAB = False\n",
    "\n",
    "if IN_COLAB:\n",
    "    drive.mount(\"/content/drive\")\n",
    "    PROJECT_ROOT = Path(\"/content/drive/MyDrive/DataScience/census-education-analysis\")\n",
    "else:\n",
    "    PROJECT_ROOT = Path.cwd().parent\n",
    "\n",
    "DATA_DIR = PROJECT_ROOT / \"data\"\n",
    "RAW_DIR = DATA_DIR / \"raw\"\n",
    "STAGING_DIR = DATA_DIR / \"staging\"\n",
    "PROCESSED_DIR = DATA_DIR / \"processed\"\n",
    "OUTPUTS_DIR = PROJECT_ROOT / \"outputs\"\n",
    "\n",
    "PROJECT_ROOT\n"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "id": "0ed7cb2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Problem 1: What Dataset Are We Interpreting?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16759aad",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "input_path = OUTPUTS_DIR / \"india_model_evaluation_ready.csv\"\n",
    "df = pd.read_csv(input_path)\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "id": "b1dc2db1",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Problem 2: What Does “Interpretability” Mean in Practice?"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "id": "d14d7d4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Problem 3: Why Simple Comparisons Reveal Model Behavior"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "412a6974",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.sort_values(\"risk_score\", ascending=False).head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc3d8cda",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.sort_values(\"risk_score\", ascending=True).head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "id": "8b5c0a9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Problem 4: How Do Features Relate to the Risk Score?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4ac97b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "df[[\n",
    "    \"literacy_rate\",\n",
    "    \"gender_literacy_gap\",\n",
    "    \"total_persons\",\n",
    "    \"risk_score\"\n",
    "]].corr()"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "id": "1fd54ec1",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Problem 5: How Do We Explain Rankings in Plain Language?"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "id": "4c8e041a",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Problem 6: How Do We Check Alignment with Human Judgment?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b59f813",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.crosstab(\n",
    "    df[\"priority_flag\"],\n",
    "    df[\"predicted_flag\"],\n",
    "    rownames=[\"Human Priority\"],\n",
    "    colnames=[\"Model Prediction\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "id": "10eb7531",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Problem 7: Why Interpretability Is More Important Than Optimization"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "id": "d6840f2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Problem 8: Capturing Interpretation Signals Explicitly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1860cc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"high_risk_band\"] = df[\"risk_score\"] >= df[\"risk_score\"].quantile(0.80)\n",
    "df[\"low_risk_band\"] = df[\"risk_score\"] <= df[\"risk_score\"].quantile(0.20)"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "id": "09f86471",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Problem 9: Saving the Interpretable Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19d4e453",
   "metadata": {},
   "outputs": [],
   "source": [
    "output_path = OUTPUTS_DIR / \"india_model_interpretable.csv\"\n",
    "df.to_csv(output_path, index=False)\n",
    "\n",
    "output_path"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "id": "2bc3a435",
   "metadata": {},
   "outputs": [],
   "source": [
    "## End-of-Chapter Direction"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
